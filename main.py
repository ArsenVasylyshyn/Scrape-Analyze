from src.scraper import get_all_vacancies_html_selenium, parse_vacancies_from_html, parse_each_vacancy
from src.sqlite_db import init_db, insert_vacancy_with_technologies
from src.export import save_to_csv
import time    

if __name__ == "__main__":
    LIMIT_ENABLED = False  # ğŸ” Ğ£Ğ²Ñ–Ğ¼ĞºĞ½Ğ¸ False, Ñ‰Ğ¾Ğ± Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´Ğ¸Ñ‚Ğ¸ Ğ²ÑÑ– Ğ²Ğ°ĞºĞ°Ğ½ÑÑ–Ñ—
    LIMIT = 40             # ğŸ”¢ Ğ¡ĞºÑ–Ğ»ÑŒĞºĞ¸ Ğ¾Ğ±Ñ€Ğ¾Ğ±Ğ»ÑÑ‚Ğ¸, ÑĞºÑ‰Ğ¾ ÑƒĞ²Ñ–Ğ¼ĞºĞ½ĞµĞ½Ğ¾ Ğ»Ñ–Ğ¼Ñ–Ñ‚

    init_db()

    html = get_all_vacancies_html_selenium(limit=LIMIT if LIMIT_ENABLED else float('inf'))
    results = parse_vacancies_from_html(html)
    print(f"ğŸ”— Ğ—Ğ°Ğ³Ğ°Ğ»Ğ¾Ğ¼ Ğ·Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾ Ğ²Ğ°ĞºĞ°Ğ½ÑÑ–Ğ¹ Ğ¿Ñ–ÑĞ»Ñ Ğ¿Ñ–Ğ´Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶ĞµĞ½Ğ½Ñ: {len(results)}")

    all_data = []
    for i, (title, company, link) in enumerate(results):
        if LIMIT_ENABLED and i >= LIMIT:
            break

        print(f"ğŸ” Company: {company}")
        print(f"ğŸ“Œ Vacancy title: {title}")
        print(f"ğŸ”— Link: {link}")

        keywords = parse_each_vacancy(link)
        insert_vacancy_with_technologies(title, company, link, keywords)

        all_data.append((title, company, link, keywords))

        print("ğŸ’¾ Ğ—Ğ±ĞµÑ€ĞµĞ¶ĞµĞ½Ğ¾ Ñƒ Ğ±Ğ°Ğ·Ñƒ")
        print("-" * 50)
        time.sleep(1)

    # Save to CSV after parsing
    save_to_csv(all_data)